{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d585f46",
   "metadata": {},
   "source": [
    "# üåø Plant Recognition with PlantCLEF 2025 on Kaggle\n",
    "\n",
    "**Purpose**: Use PlantCLEF 2025 dataset directly on Kaggle without downloading 1TB locally\n",
    "\n",
    "**Strategy**:\n",
    "1. Run this notebook on Kaggle (datasets available there)\n",
    "2. Accept user uploaded images via API\n",
    "3. Use CLIP for similarity search\n",
    "4. Return top-5 plant predictions\n",
    "5. No need for local storage or IDrive2\n",
    "\n",
    "**Dataset**: PlantCLEF 2025 (~1TB)  \n",
    "**Model**: CLIP (openai/clip-vit-base-patch32)  \n",
    "**Platform**: Kaggle Notebooks (GPU enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f73f0e",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f54b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once on Kaggle)\n",
    "!pip install -q transformers torch pillow\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"üíæ Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e5c8f1",
   "metadata": {},
   "source": [
    "## 2. Configure Kaggle Dataset Access\n",
    "\n",
    "PlantCLEF 2025 dataset is available on Kaggle. We'll access it directly from `/kaggle/input/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36010dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle dataset path (automatically mounted on Kaggle notebooks)\n",
    "DATASET_PATH = \"/kaggle/input/plantclef2025\"\n",
    "\n",
    "# Check if dataset is available\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(\"‚úÖ PlantCLEF 2025 dataset found!\")\n",
    "    \n",
    "    # List dataset contents\n",
    "    contents = os.listdir(DATASET_PATH)\n",
    "    print(f\"\\nüìÅ Dataset contents ({len(contents)} items):\")\n",
    "    for item in contents[:10]:\n",
    "        print(f\"   - {item}\")\n",
    "    if len(contents) > 10:\n",
    "        print(f\"   ... and {len(contents) - 10} more items\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset not found!\")\n",
    "    print(\"üí° Make sure to:\")\n",
    "    print(\"   1. Add 'PlantCLEF 2025' dataset to this notebook\")\n",
    "    print(\"   2. Enable GPU accelerator (Settings > Accelerator > GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b71a0",
   "metadata": {},
   "source": [
    "## 3. Load CLIP Model\n",
    "\n",
    "We'll use CLIP for zero-shot plant recognition and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9660a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and processor\n",
    "print(\"üîÑ Loading CLIP model...\")\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(f\"‚úÖ CLIP model loaded on {device}\")\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d4978e",
   "metadata": {},
   "source": [
    "## 4. Image Encoding Function\n",
    "\n",
    "Function to encode images using CLIP with L2 normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355eb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path_or_pil):\n",
    "    \"\"\"\n",
    "    Encode image using CLIP\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_pil: File path (str) or PIL Image\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Normalized 512-dimensional embedding\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    if isinstance(image_path_or_pil, str):\n",
    "        image = Image.open(image_path_or_pil).convert(\"RGB\")\n",
    "    else:\n",
    "        image = image_path_or_pil.convert(\"RGB\")\n",
    "    \n",
    "    # Process with CLIP\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = model.get_image_features(**inputs)\n",
    "        # L2 normalization for cosine similarity\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return features.cpu().numpy().flatten()\n",
    "\n",
    "# Test encoding function\n",
    "print(\"‚úÖ Image encoding function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feed503",
   "metadata": {},
   "source": [
    "## 5. User Image Upload and Processing\n",
    "\n",
    "**API Integration**: This notebook will be called from FastAPI backend.\n",
    "\n",
    "When a user uploads an image:\n",
    "1. Backend sends image to this Kaggle notebook (via Kaggle API)\n",
    "2. Notebook processes image with CLIP\n",
    "3. Searches PlantCLEF dataset for similar plants\n",
    "4. Returns top-5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process user uploaded image\n",
    "# In production, this will be called via Kaggle API\n",
    "\n",
    "def process_user_image(image_input):\n",
    "    \"\"\"\n",
    "    Process user uploaded image and find similar plants\n",
    "    \n",
    "    Args:\n",
    "        image_input: PIL Image or file path\n",
    "    \n",
    "    Returns:\n",
    "        dict: Predictions with confidence scores\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Processing user image...\")\n",
    "    \n",
    "    # Encode user image\n",
    "    user_embedding = encode_image(image_input)\n",
    "    print(f\"‚úÖ Image encoded: {user_embedding.shape}\")\n",
    "    \n",
    "    # TODO: Load pre-computed embeddings from PlantCLEF dataset\n",
    "    # For now, return mock predictions\n",
    "    predictions = [\n",
    "        {\"species\": \"Rosa damascena\", \"confidence\": 0.95, \"family\": \"Rosaceae\"},\n",
    "        {\"species\": \"Rosa gallica\", \"confidence\": 0.89, \"family\": \"Rosaceae\"},\n",
    "        {\"species\": \"Rosa canina\", \"confidence\": 0.82, \"family\": \"Rosaceae\"},\n",
    "        {\"species\": \"Hibiscus rosa-sinensis\", \"confidence\": 0.76, \"family\": \"Malvaceae\"},\n",
    "        {\"species\": \"Camellia japonica\", \"confidence\": 0.71, \"family\": \"Theaceae\"}\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"predictions\": predictions,\n",
    "        \"embedding_dim\": len(user_embedding),\n",
    "        \"processing_time\": \"~2.5s\"\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Image processing function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356eb1f5",
   "metadata": {},
   "source": [
    "## 6. Visualization (Optional)\n",
    "\n",
    "Display sample images from PlantCLEF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746eb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from dataset\n",
    "def visualize_samples(dataset_path, num_samples=6):\n",
    "    \"\"\"Display random samples from PlantCLEF dataset\"\"\"\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"‚ö†Ô∏è Dataset not found - skipping visualization\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # TODO: Load actual images from PlantCLEF\n",
    "    # For now, show placeholder\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.text(0.5, 0.5, f\"PlantCLEF Image {i+1}\\n(Load from dataset)\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Sample Plants from PlantCLEF 2025 Dataset\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13fe50",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "**To complete this notebook**:\n",
    "\n",
    "1. **Add PlantCLEF 2025 dataset** to this Kaggle notebook\n",
    "   - Go to \"Add data\" ‚Üí Search \"PlantCLEF 2025\"\n",
    "   - Add to notebook\n",
    "\n",
    "2. **Pre-compute embeddings** (one-time setup)\n",
    "   - Encode all PlantCLEF images with CLIP\n",
    "   - Save embeddings to disk (~2GB for 1M images)\n",
    "   - Load embeddings for fast similarity search\n",
    "\n",
    "3. **Implement similarity search**\n",
    "   - Use cosine similarity to find k-nearest neighbors\n",
    "   - Return top-5 predictions with confidence scores\n",
    "\n",
    "4. **API Integration**\n",
    "   - Set up Kaggle API endpoint\n",
    "   - Accept images from FastAPI backend\n",
    "   - Return predictions as JSON\n",
    "\n",
    "**Benefits of this approach**:\n",
    "- ‚úÖ No need to download 1TB dataset locally\n",
    "- ‚úÖ No IDrive2 storage needed\n",
    "- ‚úÖ Use Kaggle's free GPU for inference\n",
    "- ‚úÖ Access latest PlantCLEF updates directly"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
