import httpx
from typing import Optional
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)

class LLMService:
    """Google Gemini AI servisi (OpenRouter yerine)"""
    def __init__(self):
        # Tercih: Google AI Studio, yoksa OpenRouter
        self.google_api_key = settings.GOOGLE_AI_STUDIO_API_KEY
        self.google_model = settings.GOOGLE_AI_STUDIO_MODEL

        self.api_key = settings.OPENROUTER_API_KEY
        self.api_url = settings.OPENROUTER_BASE_URL
        self.model = settings.OPENROUTER_MODEL

        # Google Gemini client (lazy init)
        self._google_client = None

        # Default headers for OpenRouter (kept for fallback)
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "http://localhost:8000",
            "X-Title": "Plant Recognition System"
        }

    @property
    def google_client(self):
        """Lazy initialization of Google Gemini client"""
        if self._google_client is None and self.google_api_key:
            try:
                from google import genai
                self._google_client = genai.Client(api_key=self.google_api_key)
                logger.info("âœ… Google Gemini client initialized")
            except ImportError:
                logger.warning("google-genai package not installed, falling back to OpenRouter")
            except Exception as e:
                logger.error(f"Failed to initialize Google Gemini client: {e}")
        return self._google_client
    
    async def generate_response(self, prompt: str, context: Optional[str] = None) -> str:
        try:
            system_prompt = """Sen uzman bir botanik asistanÄ±sÄ±n. TÃ¼rkÃ§e olarak yanÄ±t ver.

KURALLAR:
1. SADECE TÃ¼rkÃ§e yanÄ±t ver - baÅŸka dil kullanma
2. Net, anlaÅŸÄ±lÄ±r ve bilimsel doÄŸru bilgi ver
3. Bitki adlarÄ±nÄ± format: "Bilimsel Ad (TÃ¼rkÃ§e Ä°sim)" ÅŸeklinde yaz
4. GÃ¼ven skorlarÄ±nÄ± belirt
5. KÄ±sa ve Ã¶z bilgi ver (2-3 paragraf)
6. Abartma, sadece verilen context bilgisini kullan

YASAKLAR:
- Karma dil kullanÄ±mÄ± (Korece, Japonca, Ä°ngilizce, RusÃ§a karÄ±ÅŸÄ±k)
- AnlamsÄ±z kelimeler
- Uydurma bilgi
- Ã‡ok uzun aÃ§Ä±klamalar"""
            
            if context:
                system_prompt += f"\n\nVERÄ°TABANI BÄ°LGÄ°SÄ°:\n{context}"
            
            # Compose full prompt
            full_prompt = system_prompt + "\n\nKullanÄ±cÄ± Sorusu:\n" + prompt

            # If Google AI Studio key is provided, use it (preferred)
            if self.google_client:
                logger.info(f"ğŸ¤– Using Google Gemini: {self.google_model}")
                try:
                    response = self.google_client.models.generate_content(
                        model=self.google_model,
                        contents=full_prompt
                    )
                    
                    # Extract text from response
                    if hasattr(response, 'text'):
                        logger.info("âœ… Google Gemini response received")
                        return response.text
                    elif hasattr(response, 'candidates') and len(response.candidates) > 0:
                        candidate = response.candidates[0]
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            text = ''.join(part.text for part in candidate.content.parts if hasattr(part, 'text'))
                            logger.info("âœ… Google Gemini response received (from candidates)")
                            return text
                    
                    logger.warning("âš ï¸ Google Gemini returned unexpected format, falling back to OpenRouter")
                except Exception as e:
                    logger.error(f"âŒ Google Gemini API error: {e}, falling back to OpenRouter")

            # Otherwise fallback to OpenRouter
            logger.info(f"ğŸ”„ Using OpenRouter (fallback): {self.model}")
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            payload = {
                "model": self.model,
                "messages": messages,
                "max_tokens": 400,
                "temperature": 0.3,  # Daha tutarlÄ± yanÄ±tlar iÃ§in dÃ¼ÅŸÃ¼k temperature
                "top_p": 0.9,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.api_url}/chat/completions",
                    headers=self.headers,
                    json=payload
                )
                response.raise_for_status()
                result = response.json()
                return result["choices"][0]["message"]["content"]
        except httpx.HTTPStatusError as e:
            error_detail = e.response.text if hasattr(e, 'response') else str(e)
            logger.error(f"OpenRouter API HTTP error: {e.response.status_code} - {error_detail}")
            
            # 429 Rate Limit veya diÄŸer hatalar iÃ§in fallback
            if e.response.status_code == 429:
                logger.warning("Rate limit hit, using fallback response")
            
            # Hata durumunda basit fallback (bitki verisi olmadan)
            raise  # Hata fÄ±rlat ki generate_rag_response fallback'i Ã§aÄŸÄ±rsÄ±n
        except httpx.TimeoutException:
            logger.error("OpenRouter API timeout")
            raise  # Hata fÄ±rlat ki generate_rag_response fallback'i Ã§aÄŸÄ±rsÄ±n
        except KeyError as e:
            logger.error(f"Unexpected API response format: {e}")
            raise  # Hata fÄ±rlat ki generate_rag_response fallback'i Ã§aÄŸÄ±rsÄ±n
        except Exception as e:
            logger.error(f"LLM service error: {type(e).__name__} - {str(e)}")
            raise  # Hata fÄ±rlat ki generate_rag_response fallback'i Ã§aÄŸÄ±rsÄ±n
    
    def _generate_fallback_response(self, prompt: str, plants: list = None) -> str:
        """LLM kullanÄ±lamadÄ±ÄŸÄ±nda bitki bilgileriyle zengin yanÄ±t Ã¼ret"""
        logger.info(f"Using fallback response with {len(plants) if plants else 0} plants")
        
        # EÄŸer bitki verisi varsa, detaylÄ± bilgi ver
        if plants and len(plants) > 0:
            response_parts = ["GÃ¶rsel analizi tamamlandÄ±! Ä°ÅŸte bulunan olasÄ± bitkiler:\n"]
            
            for idx, plant in enumerate(plants[:3], 1):
                sci_name = plant.get('scientificName', 'Bilinmeyen')
                common_name = plant.get('commonName', '')
                
                # Weaviate'den gelen certainty deÄŸeri _additional nesnesinde
                additional = plant.get('_additional', {})
                confidence = additional.get('certainty', plant.get('certainty', plant.get('score', 0)))
                
                # EÄŸer hala 0 ise distance'dan hesapla
                if confidence == 0 and 'distance' in additional:
                    distance = additional.get('distance', 2)
                    confidence = max(0.0, min(1.0, 1 - (distance / 2)))
                
                family = plant.get('family', '')
                
                response_parts.append(f"\n{idx}. **{sci_name}**")
                if common_name:
                    response_parts.append(f" ({common_name})")
                response_parts.append(f"\n   - GÃ¼ven skoru: {confidence:.1%}")
                if family:
                    response_parts.append(f"\n   - Familya: {family}")
                
                logger.debug(f"Fallback plant {idx}: {sci_name} - {confidence:.1%}")
            
            response_parts.append("\n\nğŸ’¡ Bu bitkilerden biri hakkÄ±nda daha fazla bilgi almak iÃ§in soru sorabilirsiniz.")
            return "".join(response_parts)
        
        # Bitki verisi yoksa genel yanÄ±t
        prompt_lower = prompt.lower()
        
        if any(word in prompt_lower for word in ['identify', 'what', 'which', 'plant', 'species']):
            return ("GÃ¶rsel analizi yapÄ±ldÄ± ancak veritabanÄ±nda eÅŸleÅŸme bulunamadÄ±. "
                   "LÃ¼tfen daha net bir fotoÄŸraf deneyin veya farklÄ± aÃ§Ä±dan Ã§ekilmiÅŸ bir gÃ¶rsel yÃ¼kleyin.")
        
        if any(word in prompt_lower for word in ['care', 'water', 'sun', 'grow', 'bakÄ±m']):
            return ("Genel bitki bakÄ±m Ã¶nerileri:\n"
                   "- ğŸ’§ DÃ¼zenli sulama (topraÄŸÄ±n nemini kontrol edin)\n"
                   "- â˜€ï¸ Yeterli gÃ¼neÅŸ Ä±ÅŸÄ±ÄŸÄ± (bitkiye gÃ¶re deÄŸiÅŸir)\n"
                   "- ğŸŒ± Ä°yi drene olan toprak kullanÄ±n\n"
                   "- ğŸŒ¡ï¸ Uygun sÄ±caklÄ±k (15-25Â°C ideal)\n\n"
                   "Daha spesifik bilgi iÃ§in bitkinin adÄ±nÄ± belirtin.")
        
        return ("Ä°steÄŸiniz iÅŸlendi. LÃ¼tfen aÅŸaÄŸÄ±daki sonuÃ§larÄ± kontrol edin veya "
               "birkaÃ§ dakika sonra tekrar deneyin.")
    
    async def generate_rag_response(self, user_query: str, retrieved_plants: list) -> str:
        """RAG: VeritabanÄ±ndan alÄ±nan bitki bilgileriyle zenginleÅŸtirilmiÅŸ yanÄ±t"""
        plant_context = []
        for plant in retrieved_plants[:3]:
            plant_info = f"- {plant.get('scientificName', 'Unknown')}"
            if plant.get('commonName'):
                plant_info += f" ({plant.get('commonName')})"
            if plant.get('family'):
                plant_info += f" - Family: {plant.get('family')}"
            plant_context.append(plant_info)
        
        context = "Relevant plants from database:\n" + "\n".join(plant_context) if plant_context else None
        
        try:
            return await self.generate_response(user_query, context)
        except Exception as e:
            # LLM baÅŸarÄ±sÄ±z olursa, bitki bilgileriyle fallback yanÄ±t Ã¼ret
            logger.warning(f"LLM failed, using fallback with plant data: {e}")
            return self._generate_fallback_response(user_query, retrieved_plants)

# Global instance (geriye dÃ¶nÃ¼k uyumluluk iÃ§in)
llm_service = LLMService()
grok_service = llm_service  # Eski kod ile uyumluluk

